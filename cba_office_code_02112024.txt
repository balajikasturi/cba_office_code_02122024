//This code is devloped by balaji kasturi for CBA
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

// Assuming SparkSession is already created
val spark = SparkSession.builder.appName("UpdateQueryLogic").getOrCreate()

// Example DataFrame (replace with your actual DataFrame)  
val data = Seq(
  (1, "user1", "2023-06-15 10:30:00", true),
  (2, "user2", "2023-06-20 14:45:00", false),
  (3, "user3", "2023-06-19 08:00:00", true)
)

val columns = Seq("user_id", "username", "last_login_timestamp", "pb_online_status")

// Create the DataFrame
var df = spark.createDataFrame(data).toDF(columns: _*)

// Add surrogate key
df = df.withColumn("surrogate_key", monotonically_increasing_id())

// Example logic to compute num_accounts
val numAccountsDF = df.groupBy("user_id").agg(count("*").alias("num_accounts"))

// Example DataFrame for last_contacted (assuming df_contacts)
val dfContacts = spark.createDataFrame(Seq(
  (1, "2023-06-19 15:30:00"),
  (2, "2023-06-21 09:00:00"),
  (3, "2023-06-18 11:20:00")
)).toDF("user_id", "last_contacted_timestamp")

// Join last_contacted DataFrame with df
val lastContactedDF = df.join(dfContacts, Seq("user_id"), "left")
  .select("user_id", "last_contacted_timestamp")

// Join num_accounts and last_contacted back to the original DataFrame
val updatedDF = df.join(numAccountsDF, Seq("user_id"), "left")
  .join(lastContactedDF, Seq("user_id"), "left")
  .withColumnRenamed("count", "num_accounts")
  .withColumnRenamed("last_contacted_timestamp", "last_contacted")

// Show the resulting DataFrame
updatedDF.show()

//////////
import org.apache.spark.sql.SparkSession

// Initialize Spark session
val spark = SparkSession.builder()
  .appName("HDFS to S3 Migration")
  .getOrCreate()

// Set AWS credentials (if needed) for accessing S3
spark.conf.set("spark.hadoop.fs.s3a.access.key", "<AWS_ACCESS_KEY>")
spark.conf.set("spark.hadoop.fs.s3a.secret.key", "<AWS_SECRET_KEY>")
spark.conf.set("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com")

// Read data from HDFS
val hdfsPath = "hdfs://<your-hdfs-namenode>/path/to/data.csv"
val df = spark.read.option("header", "true").option("inferSchema", "true").csv(hdfsPath)

// Show the DataFrame to confirm
df.show()

// Write data to S3 in CSV format
val s3Path = "s3a://<your-bucket-name>/path/to/output-data.csv"
df.write.option("header", "true").csv(s3Path)

// Optional: Perform transformations before writing
val filteredDF = df.filter("salary > 5000")
val aggregatedDF = filteredDF.groupBy("department").agg(org.apache.spark.sql.functions.avg("salary").alias("average_salary"))
aggregatedDF.write.option("header", "true").csv("s3a://<your-bucket-name>/aggregated_salary.csv")




